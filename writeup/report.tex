\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

%\usepackage{aa228-jmlr2e}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{color}

\lstset{ % General setup for the package
	language=python,
	basicstyle=\small\sffamily,
	numbers=left,
	numberstyle=\small,
%	frame=tb,
	tabsize=4,
	columns=fixed,
	showstringspaces=false,
	showtabs=false,
	keepspaces,
	commentstyle=\color{blue},
	keywordstyle=\color{purple},
	emph={function, elseif, end, true, false},
	emphstyle=\color{purple},
}
%\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
 %   T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrvnat}

% \bibliographystyle{ieeetran}

 
\begin{document}

\title{Autonomous Driving in a Roundabout With Rule-Breaking Humans}

\author{\IEEEauthorblockN{Emiko Soroka}
\IEEEauthorblockA{\textit{Department of Aeronautics \& Astronautics} \\
\textit{Stanford University}\\
esoroka@stanford.edu}
%\and
%\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
%\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
%\textit{name of organization (of Aff.)}\\
%City, Country \\
%email address or ORCID}
}

\maketitle

\begin{abstract}
While intersection are often used as a test case for interaction in autonomous driving, a roundabout is another interesting case where multiple agents interact. In this paper we design a discrete MDP to represent traversing a roundabout with other agents, learn the optimal policy for navigating a roundabout, and investigate how it changes when some agents break the rules of the road.
\end{abstract}

%\begin{IEEEkeywords}
%\end{IEEEkeywords}

\section{Introduction}
Roundabouts are relatively uncommon compared to intersections in the US,
but are rising in popularity due to their safety and environmental benefits \cite{MANDAVILLI2008135}. Studies have found that replacing signalized intersections with roundabouts improves throughput for low-speed intersections \cite{persaud2001safety} and reduces the risk of rear-end collisions \cite{saccomanno2008comparing}. One empirical study conducted in the US found that replacing intersections with roundabouts effected a 40\% reduction in crashes, with an even greater decrease in traffic injuries and deaths \cite{persaud2001safety}.

Although roundabouts are simple to navigate, with the cardinal rule being that traffic entering the roundabout yields to traffic already in the circle, human drivers who are unaware of this rule often hold up traffic by stopping in the roundabout. We are interested in two questions:
\begin{itemize}
	\item Can we train an RL agent to successfully navigate a roundabout?
	\item How do rule-breaking drivers affect the optimal policy for navigating the roundabout?
\end{itemize}

\section*{Problem Description}
We represent this problem as a Markov decision process (MDP) over discrete states and actions. 

\subsection*{States} We define three integer-valued states: $s_1$, $s_2$ and $s_3$ (Fig. \ref{fig:statespace}).
\begin{itemize}
	\item
$s_1$ has 30 values, corresponding to the rounded distance traveled along the lane.
 \item
$s_2$ is the distance between the ego vehicle and the closest vehicle that presents a collision risk. This distance ranges from $0$ to $10$. %TODO diagram.
\item
$s_3$ is the ego vehicle velocity, sorted into buckets from $0$ to $10$.
\end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{figures/statespace.pdf}
	\caption{The three discretized state space variables.}
	\label{fig:statespace}
\end{figure}

\subsection*{Actions} Our RL agent has three actions available: \verb|ZERO| (no acceleration action), \verb|ACCEL| and \verb|DECEL|. Lane-keeping is managed separately to simplify the problem, and no lane changes take place.

With this representation, there are $|S_1|\times|S_2|\times|S_3| = 3630$ states and 3 actions. This is a relatively small problem, so it is possible to learn an optimal policy by a direct method such as Bellman iteration.

\subsection*{Observations} We assume there is no noise in the system and the states are directly observed.

\subsection*{Reward} The RL agent receives a time-based reward $R(t)$ (Eq. \eqref{eq:reward}) for reaching its goal on the other side of the roundabout, where $N$ is the number of time steps in the simulation and $t$ is the step where the goal is reached. This incentivizes crossing the roundabout quickly. If a collision occurs, the agent receives a reward of $-1000$ and the simulation stops.

\begin{equation}
R(t) = \begin{cases}
10 + (N-t) & \text{ for reaching the end at step }t
\\
0.1\Delta s & \text{ for traveling $\Delta s$ in $N$ steps}
\\
-1000 & \text{ for collisions.}
\end{cases}
\label{eq:reward}
\end{equation}

\section*{Modeling Other Drivers}
To provide other agents for the simulation, we defined two fixed policies by observing human drivers at the Manzanita Field roundabout.
\subsection*{Rule-Following Driver Model} The rule-following driver model approximates the correct way to navigate a roundabout. When approaching, it yields to traffic already in the roundabout. If another vehicle is too close, it decelerates unless said vehicle is not currently in the roundabout. Otherwise, it prefers to maintain a constant speed.

This model can be represented by the code:
\begin{lstlisting}[language=python]
function get_lawful_action(model)
    # If distance to another car is < 2
    # and we are NOT in the roundabout
    if model.s2 < 2 &&
       (model.s1 < L || model.s1 > 2*L)
        return :DECEL
    # maintain speed
    elseif model.s3 >= 10
        return :ZERO
    # speed up to desired speed
    else
        return :ACCEL
    end
end
\end{lstlisting}

\subsection*{Rule-Breaking Driver Model}
The rule-breaking driver model behaves like the safe driver model with probability $p \in [0,1]$. With probability $1-p$ it may take a rule-breaking action such as:
\begin{itemize}
	\item Yielding to a vehicle waiting to enter the roundabout.
	\item Not yielding to another vehicle when entering the roundabout.
\end{itemize}
This model can be represented by the code:
\begin{lstlisting}[language=python]
function get_lawless_action(model)
    # Should we yield when entering??
    if rand() < model.p && model.s2 < 2 &&
      (model.s1 == 1 || model.s1 == 3)
        return :DECEL
    # Should we stop in the roundabout??
    elseif rand() > model.p && model.s2 < 2
        returj :DECEL
    # stay at desired speed
    elseif model.s3 >= 10
        return :ZERO
    # speed up if too slow
    else
        return :ACCEL
    end
end
\end{lstlisting}
\begin{figure}[h!]
	\centering
\includegraphics[width=0.7\linewidth]{figures/unsafe.png}
\caption{The unsafe driver (gray) enters the roundabout in front of the rule-following green car, causing a collision.}
\label{fig:unsafe}
\end{figure}

\section{Methodology}
We used \verb|AutomotiveSimulator.jl| \cite{simulator.jl} to define the environment, rule-breaking and rule-following agents, and the RL agent itself.
We used online Bellman iteration \cite{kochenderfer2022algorithms} to learn the roundabout policy by running the simulation with varying starting positions and other agents.
We studied two cases:

\subsection*{All safe drivers:} The other drivers all follow the rules, yielding and proceeding correctly. Since Bellman iteration is exact, we expect this to converge to an optimal policy for navigating the roundabout.

\subsection*{Some unsafe drivers:} A fraction $r\in (0,1)$ of the drivers break the rules with probability $p$. We used $p=0.5$, $r=0.25$ and $r=0.5$.

\subsection*{Exploration:} We initially tested several different methods of balancing exploration vs exploitation and settled on a constant 50\% chance of taking a random action during training.

Other approaches tested included:
\begin{itemize}
	\item Greedy selection: this did not work because the agent did not explore enough of the state space and never learned to move forward.
	\item Decreasing the exploration over time: this turned out to be unnecessary.
\end{itemize}

\section{Implementation} Most of the work on this project was in coming up with a reasonable discretization of the state space and debugging issues related to reward and simulation design.

\subsection*{State Space and Reward} The space was chosen to be large enough to capture the desired behavior while still being tractable, and various modifications had to be made to resolve problems that arose during training.
For example, we initially tried to use three states to capture position: before, in, and after the roundabout. This failed because the RL agent would take an action and remain in the same state, so it wasn't able to train.

We also tested several reward functions before settling on an appropriate one - some tests only gave a positive reward for crossing the roundabout, but the agent wasn't able to reach it before the simulation ended and again, didn't train effectively.

\subsection*{Simulation Design} A final major issue was preparing simulations that effectively explored the state space. Initial simulations used three other agents, one for each direction, but this resulted in very few observed states where a collision was possible. The resulting agent was unlikely to decelerate and avoid collisions.

\subsection*{Collision Course Detection}
A major challenge was defining and fine-tuning the human driver models for the RL agent to train with, especially correctly implementing the yielding behavior. Many challenges also arose in identifying whether two vehicles are on a collision course. Two examples of this situation are shown in Figure \ref{fig:collision}.

\begin{figure}[h!]
	\centering
\includegraphics[width=0.8\linewidth]{figures/example_roundabout.pdf}
\caption{Vehicle F is on a collision course with E. C is on a collision course with B, but B is not on a collision course with C (because C should yield).}
\label{fig:collision}
\end{figure}

The collision course detection was eventually implemented with a special case for the roundabout.
\begin{lstlisting}[language=python]
function on_coll_course(v1::Entity, v2::Entity)
	# check projection of velocity vectors
	vel1 = velg(v1.state)
	dx = (posg(v2.state) .- posg(v1.state))[1:2]
	projection = dot(dx, vel1)
	if projection > 5.0 && projection < 150
		return true
	end
	# SPECIAL CASE: if we are not in
	# the roundabout and the other vehicle is
	s1 = posf(v1.state).s
	s2 = posf(v2.state).s
	if (s1 < L|| s1 > 2L) &&
	   (s2 >= L && s2 <= 2L)
		return true
	end
	return false
end
\end{lstlisting}


\section{Results}
Table \ref{tab:agent_results} shows the reward obtained over an average of 10 trials for each trained Rl agent. The trials were sorted into three categories based on outcome: completed trials (the agent successfully crossed the roundabout), passed trials (didn't cross, but didn't collide) and collisions.

The first three entries are for the hand-coded rule-following agent to provide a baseline. We see that the agent is imperfect (likely due to simplifications made in its design), as even when all agents follow the rules, one collision was observed.

As expected, the number of collisions increases as the percentage of unsafe drivers increases. Interestingly, for successful runs, the completed score is very similar for agents trained with only safe drivers and agents trained with some unsafe drivers.

Our hypothesis was that unsafe drivers would result in a slower policy that receives less reward, however this doesn't seem to be the case. This could be due to a bug (especially in the function that determines whether two vehicles are at risk of colliding) or a poorly designed reward function.

\begin{figure}[h]
	\centering
	\includegraphics[width=1.0\linewidth]{figures/final_convergence.pdf}
	\caption{Convergence of four test cases for the final RL agent.}
	\label{fig:convergence}
\end{figure}

We used 50 runs per epoch, checking the convergence condition $\|U_k-U_{k-1}\|_2 / \|U_k\|_2 <\epsilon$ at the end of each epoch. For all agents, $\epsilon$ was set to $5e^{-4}$.

The addition of unsafe drivers decreased the convergence rate (Figure \ref{fig:convergence}). However, all four agents were able to converge to a reasonably effective policy.
\clearpage

%TODO baseline with all 4 human agents in this table
\begin{table}[h!]
	\begin{minipage}{\textwidth}
		\centering
		\begin{tabular}{c|p{5em}p{9em}ccc}
			Agent & Training exploration & Percent rule-breaking agents in simulation & \# Completed trials & Passed trials & Collisions
			\\\hline
			Rule-following &N/A & 0\% & 136.67 & N/A & 1 \\
			Rule-following &N/A & 25\% & 139.2 & N/A & 1 \\
			Rule-following &N/A & 50\% & 137.83 & N/A &4\\
			\verb|exp_03_unsafe_0| &0.3 & 0\%& 111.5 & 4.68 & 1\\
			\verb|exp_05_unsafe_0| &0.5 &0\% &107.5 & 3.04 & 1 \\
			\verb|exp_05_unsafe_025| &0.5 &25\% & 113.5 & 3.57 & 3 \\
			\verb|exp_05_unsafe_05| &0.5 &50\%& 113.25 & 2.39 & 5 \\
		\end{tabular}
		\caption{Simulation results for various RL agents, choosing the optimal action (no exploration). ``Completed" runs result in the agent successfully crossing the roundabout. ``Passed" runs avoid a collision but do not cross the roundabout.}
		\label{tab:agent_results}
	\end{minipage}
\end{table}

\subsection*{Comparison against Baseline} We can compare the performance of our hand-coded human driver with the various RL agents in Table \ref{tab:agent_results}.
The RL agent performs similarly to the hand-coded agent, suggesting it is able to learn a reasonable policy. Both agents suffer collisions when rule-breaking drivers are added to the roundabout, suggesting improved collision avoidance would be a good next step.

Inspecting the policy learned by the RL agents reveals many states remain unexplored.
% TODO finish here
% TODO add some visualizations of the policy and then we are DONE.

\section*{Acknowledgments}
I would like to thank Prof. Kochenderfer and all of his TAs for being great this quarter, teaching a lot of interesting material and giving us the freedom to be creative with our final projects.

SISL developed the AutomotiveSimulator.jl package which provided a reliable base for RL training and simulations.

Finally, my original proposal was to use MDPs for modeling intersections. Credit for the switch to a more interesting topic goes to all of the sub-optimal drivers holding up traffic in Stanford's roundabouts.
%\vspace{1em}\\
%~
%\\
%\vspace{10em}

\bibliography{../bibliography}
\end{document}
